{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3aac94-1bd0-411c-9fd7-8b0dfeb252c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor, HistGradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import Lasso, LassoCV, SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from joblib import dump, load\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e728c2-b41c-4113-849f-7aab7d0f73df",
   "metadata": {},
   "source": [
    "## Dataset set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ab029-fe53-471d-83a3-d8acf9f419e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('gs://carddefaultdataset/playground-series-s4e5/playground-series-s4e5/train.csv')\n",
    "train_df = train_df.drop('id', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5591a3fa-6d44-4d38-8a79-3bf78f1c1f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_y = train_df['FloodProbability']\n",
    "train_x = train_df.drop('FloodProbability', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ab65c-1463-469a-8763-d947331cbeac",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "looking at the data to see if anything interesting comes up to give us ideas for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c42bc-f92a-4fae-b1d8-895237340216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93146146-3cac-493f-abbd-649b156b6f00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.hist(bins=15, figsize=(15, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4847c2-5534-4ebe-96a1-f09136170449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(train_df.corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6eeebf-3ea3-48cc-82c0-d02189a51dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.plot(kind='box', subplots=True, layout=(7,3), figsize=(15, 10), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d09211b-4427-4339-9eae-8ca876a4cb7d",
   "metadata": {},
   "source": [
    "## EDA Results\n",
    "I think this is a synthetic dataset, so not really anything interesting! The distributions generally are normal, but there are some weird skews in some of them. However, as we're using tree-based methods, this doesn't really matter (but normalization might be useful for other methods) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24168f5-6cef-49e6-ac26-074e57568844",
   "metadata": {},
   "source": [
    "## Experiment 1 - Forest on all features\n",
    "First we use just a random forest, with grid search to find the best hyperparameters. We also use this as one submission, and get not very good results. Possible/probable that we didn't allow enough estimators, due to memory restrictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8fb854-8393-470d-b6f5-ee15cbf5b058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(verbose = 1)\n",
    "param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [4, 8, 16]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e0409-747f-4e04-a2e6-a048cc69a656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=regressor, param_grid=param_grid, n_jobs = -1)\n",
    "grid_search.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f4799-0f68-43d5-8634-576071bb9e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b88605-af42-4858-a157-4709fece5ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_forest = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f64f30-8f7e-48e1-aaff-098bcf23a2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('gs://carddefaultdataset/playground-series-s4e5/playground-series-s4e5/test.csv')\n",
    "test_ids = test_df['id']\n",
    "test_x = test_df.drop('id', axis = 1)\n",
    "test_y = best_forest.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff9aa3-c8d8-4611-a7c1-7fc5264fe486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'FloodProbability': test_y\n",
    "})\n",
    "\n",
    "result_df.to_csv('rf_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e11307-83f0-4de9-afde-7923425f2183",
   "metadata": {},
   "source": [
    "## Results of Experiment 1\n",
    "Accuracy with random forest with 16 depth and 200 predictors (best performing forest) using all features was very poor, 0.58698"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d3c73-eb91-4eb2-972c-05613837ba9c",
   "metadata": {},
   "source": [
    "## Experiment 4 - Bigger forests\n",
    "Same as experiment 1 but with many more predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15701b3-b341-4917-8c48-f7c4236eaff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(verbose = 1)\n",
    "param_grid = {'n_estimators': [200, 300, 400], 'max_depth': [4, 8, 16]}\n",
    "grid_search = GridSearchCV(estimator=regressor, param_grid=param_grid, n_jobs = -1)\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766daa95-d44b-4832-a6dc-0aeebf90c302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_forest = grid_search.best_estimator_\n",
    "test_df = pd.read_csv('gs://carddefaultdataset/playground-series-s4e5/playground-series-s4e5/test.csv')\n",
    "test_ids = test_df['id']\n",
    "test_x = test_df.drop('id', axis = 1)\n",
    "test_y = best_forest.predict(test_x)\n",
    "result_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'FloodProbability': test_y\n",
    "})\n",
    "\n",
    "result_df.to_csv('brf_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d7103-c4ce-489d-8fbc-1a660a70403e",
   "metadata": {},
   "source": [
    "## Experiment 4 results\n",
    "not much better than experiment 1. 0.58749 accuracy. The grid search is still telling us that the best performance is coming from the most complext models, but we seem to be getting a bit of a slow down in return on investment. So, for now, we'll stop with random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb2f045-fa28-4b51-ab48-0facc92310b4",
   "metadata": {},
   "source": [
    "## Experiment 5 - Boosting\n",
    "Then we take a gradient boosting approach, again using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980dbee-35d5-4b86-b472-d0410b030813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boosted_regressor = GradientBoostingRegressor(verbose = 1)\n",
    "boosted_param_grid = {'n_estimators': [100, 200, 300], 'learning_rate': [0.01, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6c52c-c2b8-4d3d-9d70-9445a3e39832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boosted_grid_search = GridSearchCV(estimator=boosted_regressor, param_grid=boosted_param_grid, n_jobs = -1)\n",
    "boosted_grid_search.fit(train_x, train_y)\n",
    "print(\"Best parameters:\", boosted_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69723f1-4081-4b11-8b2b-eeca0312ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", boosted_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4ee29-2b3b-4968-976a-964f32ea02f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boosted_estimator = boosted_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be0789-7324-4e8e-b9ab-54a1dab2f643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('gs://carddefaultdataset/playground-series-s4e5/playground-series-s4e5/test.csv')\n",
    "test_ids = test_df['id']\n",
    "test_x = test_df.drop('id', axis = 1)\n",
    "test_y = boosted_estimator.predict(test_x)\n",
    "result_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'FloodProbability': test_y\n",
    "})\n",
    "result_df.to_csv('boosted_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b09e0-c581-4941-9672-08988572224b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimal_estimators = boosted_estimator.n_estimators_\n",
    "print(optimal_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e4d69-aec0-42fb-a29b-d335a1a7359f",
   "metadata": {},
   "source": [
    "## Experiment 5 Results\n",
    "Much better accuracy than random forests, we're now up to 0.81 accuracy. However, from grid search and retraining, we can see that all estimators are being used. So we are going to try with more estimators, but the same learning rate (0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b3246-f62e-4b51-8e20-4fa237e3a156",
   "metadata": {},
   "source": [
    "## Experiment 6 - Bigger boosting\n",
    "Boosting with more estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7609d0dc-c5b7-459d-8b54-6dddff8ede6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boosted_regressor = GradientBoostingRegressor(learning_rate = 0.1)\n",
    "boosted_param_grid = {'n_estimators': [300, 500, 1000]}\n",
    "boosted_grid_search = GridSearchCV(estimator=boosted_regressor, verbose = 2, param_grid=boosted_param_grid, n_jobs = -1)\n",
    "boosted_grid_search.fit(train_x, train_y)\n",
    "print(\"Best parameters:\", boosted_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a836de3-f831-4876-8a5a-66f5dc8066c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boosted_estimator = boosted_grid_search.best_estimator_\n",
    "optimal_estimators = boosted_estimator.n_estimators_\n",
    "print(optimal_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a65dd7-2eb9-47e6-85f2-451a27496250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('gs://carddefaultdataset/playground-series-s4e5/playground-series-s4e5/test.csv')\n",
    "test_ids = test_df['id']\n",
    "test_x = test_df.drop('id', axis = 1)\n",
    "test_y = boosted_estimator.predict(test_x)\n",
    "result_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'FloodProbability': test_y\n",
    "})\n",
    "result_df.to_csv('bboosted_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39f28c-ec56-4859-bd9b-75abaf4161fc",
   "metadata": {},
   "source": [
    "## Experiment 6 Results\n",
    "Better than experiment 5 - we got 0.84 accuracy now. However, again it just goes to the most complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89116d1-143a-4f33-8a90-8fb7b0f88a81",
   "metadata": {},
   "source": [
    "## Experiment 7 - Stacking\n",
    "Stacking the booster with the random forest. Using the best params from the grid search but taking into account model complexity. We therefore don't use 400 estimators for random forest. SVR as final predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86f179-4044-4107-8da2-6e3af2842f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regressor1 = GradientBoostingRegressor(n_estimators = 1000, learning_rate = 0.1, verbose = 1)\n",
    "regressor2 = RandomForestRegressor(n_estimators = 200, max_depth = 16, verbose = 1)\n",
    "\n",
    "estimators = [('GB', regressor1), ('RF', regressor2)]\n",
    "stacked_regressor = StackingRegressor(estimators = estimators, final_estimator = SVR(), verbose = 4, n_jobs = -1)\n",
    "\n",
    "stacked_regressor.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84331e5-5627-4569-9598-5a9fc161715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('gs://carddefaultdataset/playground-series-s4e5/playground-series-s4e5/test.csv')\n",
    "test_ids = test_df['id']\n",
    "test_x = test_df.drop('id', axis = 1)\n",
    "test_y = stacked_regressor.predict(test_x)\n",
    "result_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'FloodProbability': test_y\n",
    "})\n",
    "result_df.to_csv('stacked_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbec119-b310-480b-935d-efe72bad982c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forest = stacked_regressor.named_estimators_['RF']\n",
    "boosted = stacked_regressor.named_estimators_['GB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa6ea4-fa6d-4a98-9fc3-a83dda8fcea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dump(forest, 'random_forest_model.joblib')\n",
    "dump(boosted, 'gradient_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f3fbc-e488-44d5-8316-87e3d96ad27b",
   "metadata": {},
   "source": [
    "## Experiment 7 results\n",
    "Worse performance from stacking than from just boosting. From stacking we got 0.59 accuracy, which is just slightly better than random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb8fcc-1923-4f02-9881-fdb12f0fe9dc",
   "metadata": {},
   "source": [
    "## Experiment 8 - Other boosting\n",
    "Gradientboosting has only got us to 0.84 accuracy using sklearns gradientboostingregressor. We are going to try gradient histogram boosting and adaboost. First, grid search to find the best hyperparameters for each, then stacking them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a8ce6-de4f-45eb-a736-af378f251cfb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_regressor = HistGradientBoostingRegressor()\n",
    "param_grid = {'max_iter': [100,200,300,500,1000], 'learning_rate': [0.01,0.1]}\n",
    "hist_grid_search = GridSearchCV(estimator=hist_regressor, param_grid=param_grid, n_jobs = -1)\n",
    "hist_grid_search.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best parameters:\", hist_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5990a-7bef-4f6d-89b1-c20ec2c4ae79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('gs://carddefaultdataset/playground-series-s4e5/playground-series-s4e5/test.csv')\n",
    "test_ids = test_df['id']\n",
    "test_x = test_df.drop('id', axis = 1)\n",
    "test_y = hist_grid_search.predict(test_x)\n",
    "result_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'FloodProbability': test_y\n",
    "})\n",
    "result_df.to_csv('hist_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae9db2-81bd-4330-9d44-dc1d23964c1c",
   "metadata": {},
   "source": [
    "Accuracy was about 0.835, and there wasn't a substantial difference between 300 trees and 1000 trees in terms of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fa5cf-a920-46a5-ac6b-c369e486cff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ada_regressor = AdaBoostRegressor()\n",
    "param_grid = {'n_estimators': [50,100,200,300]}\n",
    "ada_grid_search = GridSearchCV(estimator=ada_regressor, param_grid=param_grid, n_jobs = -1)\n",
    "ada_grid_search.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best parameters:\", ada_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea5625-3967-430d-aa4c-c4a876c13137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('gs://carddefaultdataset/playground-series-s4e5/playground-series-s4e5/test.csv')\n",
    "test_ids = test_df['id']\n",
    "test_x = test_df.drop('id', axis = 1)\n",
    "test_y = ada_grid_search.predict(test_x)\n",
    "result_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'FloodProbability': test_y\n",
    "})\n",
    "result_df.to_csv('ada_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e42372-52b3-41e8-8986-40a6d4bf9f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dump(hist_grid_search, 'hist_boost_model.joblib')\n",
    "dump(ada_grid_search, 'ada_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec61d87-8b84-4708-ac86-b16d7980bb47",
   "metadata": {},
   "source": [
    "Ada actually performed a bit worse than even random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedac982-42c7-43f7-b66a-0d721baecbf9",
   "metadata": {},
   "source": [
    "## Experiment 8 Results\n",
    "Ada and Hist boosting both actually performed slightly worse on the dataset than the previous gradient boosting effort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f977a0-80e0-4e92-a722-97a21b8e4a83",
   "metadata": {},
   "source": [
    "## Experiment 9 - Stacking boosted results\n",
    "We have two boosted predictors which get 0.84 accuracy - we will now stack them with another model to see if we can get any better results. The relationship between the outputs of one model and the outputs of another model are obviously related, but likely in a non-linear way. To deal with non linearity, we would like to use kernels but the dataset is massive so we would be here forever. So we will approximate the RBF kernel, then train a linear model with that. We will use a validation set from training, rather than using the test data (this is due to kaggle limits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa358249-f4ab-4498-8365-4795f2c34f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_boosted_model = load('gradient_model.joblib')\n",
    "hist_boosted_model = load('hist_boost_model.joblib')\n",
    "models = [original_boosted_model, hist_boosted_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977354ab-85dd-496d-b535-7cd3b984dae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating new training dataset for SVR\n",
    "result_df = pd.DataFrame()\n",
    "for i in range(2):\n",
    "    meta_x = models[i].predict(train_x)\n",
    "    result_df[str(i)] = meta_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb781648-892e-4059-aafa-6fa2abf3c299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_train_x, meta_test_x, meta_train_y, meta_test_y = train_test_split(result_df, train_y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a57077-b835-4b96-814b-b26b8bd00498",
   "metadata": {},
   "source": [
    "Grid search with rbf approximator pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e75623-5305-4102-996a-a18bdd88bb3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rbf_pipeline = Pipeline([\n",
    "    ('rbf_sampler', RBFSampler()),\n",
    "    ('sgd_regressor', SGDRegressor())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'rbf_sampler__gamma': [0.1, 0.5, 1.0, 2.0, 'scale'],\n",
    "    'sgd_regressor__alpha': [0.0001, 0.001, 0.01],\n",
    "    'sgd_regressor__max_iter': [1000, 5000, 10000]\n",
    "}\n",
    "\n",
    "rbf_grid_search = GridSearchCV(rbf_pipeline, param_grid, verbose=2, n_jobs=-1)\n",
    "rbf_grid_search.fit(meta_train_x, meta_train_y)\n",
    "\n",
    "print(\"Best parameters:\", rbf_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaeafd5-8e1a-4135-b637-39c1241bb962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_test_y_hat = rbf_grid_search.predict(meta_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986341e-6939-4252-b6d8-92ad383d716b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(f'with the meta predictor we have {r2_score(meta_test_y, meta_test_y_hat)}')\n",
    "print('with first predictor (original) we have ' \\\n",
    "      + str(r2_score(meta_test_y, meta_test_x['0'])))\n",
    "print('with second predictor (histogram) we have ' \\\n",
    "      + str(r2_score(meta_test_y, meta_test_x['1'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824339f-ca99-488e-81c9-89af23c24a39",
   "metadata": {},
   "source": [
    "We have some actual improvement using this method, so now we will stack it all together and try it on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4852663-1e12-4563-a755-e0f11fb43e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rbf_predict(x):\n",
    "    first_pred = original_boosted_model.predict(x)\n",
    "    second_pred = hist_boosted_model.predict(x)\n",
    "    total = pd.DataFrame({\n",
    "        '0': first_pred,\n",
    "        '1': second_pred\n",
    "    })\n",
    "    return(rbf_grid_search.predict(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1620b80-34f6-4596-8082-e6552fd87232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('gs://carddefaultdataset/playground-series-s4e5/playground-series-s4e5/test.csv')\n",
    "test_ids = test_df['id']\n",
    "test_x = test_df.drop('id', axis = 1)\n",
    "test_y = rbf_predict(test_x)\n",
    "result_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'FloodProbability': test_y\n",
    "})\n",
    "result_df.to_csv('rbf_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f0113-a47d-4071-8df5-916fe3275bea",
   "metadata": {},
   "source": [
    "## Experiment 10 - Branching out from sklearn\n",
    "Tabular data uses xgboost pretty often. So, we're going to use xgboost and lgbm and then maybe do the same stacking approach as the previous experiment. We will experiment first with a train test split, then retrain on the whole dataset and use the test data. We use grid search again to find the best hyperparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb11bd7-f842-465a-9ca0-fb8cfc98ac8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa21227-a7ff-47f3-8ec4-4b58623227c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "xgb_train_x, val_x, xgb_train_y, val_y = train_test_split(train_x, train_y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f94b4d-c046-4bff-a55d-e9c1fc28abef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', nthread=-1)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.05],\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'subsample': [0.8, 1],\n",
    "    'colsample_bytree': [0.8, 1],\n",
    "}\n",
    "\n",
    "xgb_grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d85d3b-d567-4930-97ec-e7e1bb8219d5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_grid_search.fit(xgb_train_x, xgb_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f4f655-a41f-4920-8e0d-0122d466f6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(r2_score(val_y, xgb_grid_search.predict(val_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a428e-6cec-4a8a-9708-b0b489888f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#saving the xgboost model trained on part of the training data\n",
    "joblib.dump(xgb_grid_search.best_estimator_, 'partial_xgboost_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6ded1-2c6e-473f-a02e-cd915763c9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('best xgboost params are: ' + str(xgb_grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d99262-a8ec-4d7b-a4e3-65383e45927f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_model = joblib.load('partial_xgboost_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0565b195-241c-4dd3-bd73-b80ff75857e3",
   "metadata": {},
   "source": [
    "best xgboost params are: {'colsample_bytree': 1, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936e49b-e444-4946-9f30-163adc2358e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgbm_model = lgb.LGBMRegressor(n_jobs = -1)\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'max_depth': [-1, 5, 10],\n",
    "    'subsample': [0.8, 1], \n",
    "    'colsample_bytree': [0.8, 1.0]  \n",
    "}\n",
    "\n",
    "lgbm_grid_search = GridSearchCV(estimator=lgbm_model, param_grid=param_grid, scoring='neg_mean_squared_error', verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0200609-6e2a-479d-9a77-afb5189afefa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgbm_grid_search.fit(xgb_train_x, xgb_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa5d2c-5646-4b5f-81d6-3577594a098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lgbm_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddaac4-5d1d-478f-a1aa-75164bd4786a",
   "metadata": {},
   "source": [
    "The best lgbm params were found to be {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'num_leaves': 31, 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a679ba3-2a21-4e39-b1ac-0f85804f25a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joblib.dump(lgbm_grid_search.best_estimator_, 'partial_lgbm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c7f26-abea-41a7-92ba-05ae96e634da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(r2_score(val_y, lgbm_grid_search.predict(val_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f688f-e04d-4350-8f9d-b7a81bc3879b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgbm_model = joblib.load('partial_lgbm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080145d2-5fc8-4686-9097-41cd9f52bbf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rbf_pipeline = Pipeline([\n",
    "    ('rbf_sampler', RBFSampler()),\n",
    "    ('sgd_regressor', SGDRegressor())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'rbf_sampler__gamma': [0.1, 0.5, 1.0, 2.0, 'scale'],\n",
    "    'sgd_regressor__alpha': [0.0001, 0.001, 0.01],\n",
    "    'sgd_regressor__max_iter': [1000, 5000, 10000]\n",
    "}\n",
    "\n",
    "meta_train_x = pd.DataFrame({\n",
    "    '0': xgb_model.predict(xgb_train_x),\n",
    "    '1': lgbm_model.predict(xgb_train_x)\n",
    "})\n",
    "\n",
    "rbf_grid_search = GridSearchCV(rbf_pipeline, param_grid, verbose=2, n_jobs=-1)\n",
    "rbf_grid_search.fit(meta_train_x, xgb_train_y)\n",
    "\n",
    "print(\"Best parameters:\", rbf_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5322c3-fc4d-4f7f-b8d2-79853b5fb0a7",
   "metadata": {},
   "source": [
    "Best parameters for the pipeline were found to be: Best parameters: {'rbf_sampler__gamma': 'scale', 'sgd_regressor__alpha': 0.0001, 'sgd_regressor__max_iter': 5000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd597c-9df9-430e-890b-3dde490256c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rbf_predict(x, model_1, model_2, final_predictor):\n",
    "    first_pred = model_1.predict(x)\n",
    "    second_pred = model_2.predict(x)\n",
    "    total = pd.DataFrame({\n",
    "        '0': first_pred,\n",
    "        '1': second_pred\n",
    "    })\n",
    "    return(final_predictor.predict(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e70ee1-2d29-4199-9d0b-2530dec6d5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(r2_score(val_y, rbf_predict(val_x, xgb_model, lgbm_model, rbf_grid_search)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd91b4e-4157-4c87-a799-fa6fba3d0890",
   "metadata": {},
   "source": [
    "We got 0.85 r2 score now, which is the best we've had so far. We will now train xgboost and lgbm using the same hyperparams but on the entire training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828f071-3038-477f-a953-f03ed50b476e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', nthread=-1, colsample_bytree= 1, learning_rate= 0.1, max_depth= 3, n_estimators= 1000, subsample = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab04fc8-42f5-42bb-827c-dba75309370c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7770e3-c147-4525-b7d0-c178b791995a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joblib.dump(xgb_model, 'full_xgboost_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a213c13-79c1-4a76-aac6-cf2c717b0903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgbm_model = lgb.LGBMRegressor(n_jobs = -1, colsample_bytree= 0.8, learning_rate= 0.05, max_depth= 5, n_estimators= 1000, num_leaves= 31, subsample= 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95967c-bf90-441f-87b1-2b4e19ac7cc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgbm_model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02764115-0a86-47d2-96e7-9032390fa653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rbf_pipeline = Pipeline([\n",
    "    ('rbf_sampler', RBFSampler(gamma = 'scale')),\n",
    "    ('sgd_regressor', SGDRegressor(alpha = 0.0001, max_iter = 5000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366b84de-a8cb-4c2b-85a0-7f476153c036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total = pd.DataFrame({\n",
    "        '0': xgb_model.predict(train_x),\n",
    "        '1': lgbm_model.predict(train_x)\n",
    "    })\n",
    "\n",
    "rbf_pipeline.fit(total, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93781d21-6a33-4ff1-b22a-69158d415cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('gs://carddefaultdataset/playground-series-s4e5/playground-series-s4e5/test.csv')\n",
    "test_ids = test_df['id']\n",
    "test_x = test_df.drop('id', axis = 1)\n",
    "test_y = rbf_predict(test_x, xgb_model, lgbm_model, rbf_pipeline)\n",
    "result_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'FloodProbability': test_y\n",
    "})\n",
    "result_df.to_csv('xg_and_lgbm_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd08b70c-495b-47e4-b340-5cfb59a0346c",
   "metadata": {},
   "source": [
    "## Experiment 10 results\n",
    "On the public leaderboard we now are up to 85% r2 score. This is just 2% off the highest score!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16225f-0abd-4e76-92b5-679ee712a2bd",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Using tree methods is definitely the way to approach this problem. Unfortunately, the dataset is synthetic, and seems to have been synthesised in a relatively simple way. This makes it so that we can't do any really interesting feature engineering. I've spent a lot of money on google cloud resources doing this, so this is where i'll call it a day!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570676f-c24e-4338-ae5a-5dc584a16b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
